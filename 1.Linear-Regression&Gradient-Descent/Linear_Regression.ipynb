{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2760d93c",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0435a69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6c2e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Dataset\n",
    "X = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "y = [910, 1477, 1540, 2222, 2075, 2767, 3524, 3242, 3968, 4133, 5421, 5171]\n",
    "\n",
    "# Creating a DataFrame\n",
    "df = pd.DataFrame(zip(X,y), columns=['X', 'y'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d57b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Dataset\n",
    "plt.figure(figsize=(8, 6), dpi=80)\n",
    "plt.title('Linear Regression')\n",
    "plt.scatter(X,y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820bf61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Dataset\n",
    "X = np.array([1,2,3,4,5,6,7,8,9,10,11,12]).reshape((-1,1))\n",
    "y = np.array([910, 1477, 1540, 2222, 2075, 2767, 3524, 3242, 3968, 4133, 5421, 5171])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ee0e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea6bfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f99622",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_sq = model.score(X, y)\n",
    "print('coefficient of determination:', r_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dca6736",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('intercept:', model.intercept_)\n",
    "print('slope:', model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb89562",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)\n",
    "print('predicted response:', y_pred, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c5cb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.intercept_ + model.coef_ * X\n",
    "print('predicted response:', y_pred, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e9d670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot outputs\n",
    "plt.figure(figsize=(8, 6), dpi=80)\n",
    "plt.title('Linear Regression')\n",
    "plt.scatter(X, y,  color='blue')\n",
    "plt.plot(X, y_pred, color='red', linewidth=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265e2adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Original Dataset\n",
    "X = np.array([1,2,3,4,5,6,7,8,9,10,11,12]).reshape((-1,1))\n",
    "y = np.array([910, 1477, 1540, 2222, 2075, 2767, 3524, 3242, 3968, 4133, 5421, 5171])\n",
    "\n",
    "# fit Linear Regression model to the data\n",
    "model = LinearRegression().fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Plot outputs\n",
    "plt.figure(figsize=(8, 6), dpi=80)\n",
    "plt.title('Linear Regression')\n",
    "plt.scatter(X, y,  color='blue', label='training data')\n",
    "plt.plot(X, y_pred, color='red', label='linear regression', linewidth=2)\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f45809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting new variable \n",
    "a = np.array(7).reshape(-1,1)\n",
    "y_pred2 = model.predict(a)\n",
    "print('predicted response:', y_pred2, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc2fac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92824ed8",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860b084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sklearn\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Gradient Descent function\n",
    "def gradient_descent(alpha, x, y, ep=0.0001, max_iter=10000):\n",
    "    converged = False\n",
    "    iter = 0\n",
    "    m = x.shape[0] # number of samples\n",
    "\n",
    "    # initial theta\n",
    "    t0 = np.random.random(x.shape[1])\n",
    "    t1 = np.random.random(x.shape[1])\n",
    "\n",
    "    # total error, J(theta)\n",
    "    J = sum([(t0 + t1*x[i] - y[i])**2 for i in range(m)])\n",
    "\n",
    "    # Iterate Loop\n",
    "    while not converged:\n",
    "        # for each training sample, compute the gradient (d/d_theta j(theta))\n",
    "        grad0 = 1.0/m * sum([(t0 + t1*x[i] - y[i]) for i in range(m)]) \n",
    "        grad1 = 1.0/m * sum([(t0 + t1*x[i] - y[i])*x[i] for i in range(m)])\n",
    "\n",
    "        # update the theta_temp\n",
    "        temp0 = t0 - alpha * grad0\n",
    "        temp1 = t1 - alpha * grad1\n",
    "    \n",
    "        # update theta\n",
    "        t0 = temp0\n",
    "        t1 = temp1\n",
    "\n",
    "        # mean squared error\n",
    "        e = sum([(t0 + t1*x[i] - y[i])**2 for i in range(m)]) \n",
    "\n",
    "        if abs(J-e) <= ep:\n",
    "            print('Converged, iterations: ' + str(iter) + str('!!!'))\n",
    "            converged = True\n",
    "        \n",
    "        J = e   # update error \n",
    "        iter += 1  # update iter\n",
    "        \n",
    "        if iter == max_iter:\n",
    "            print('Max interactions exceeded!')\n",
    "            converged = True\n",
    "    \n",
    "    return t0,t1\n",
    "\n",
    "# Create sample data\n",
    "x, y = make_regression(n_samples=100, n_features=1, n_informative=1,\n",
    "                       random_state=0, noise=20)\n",
    "print('x.shape = %s y.shape = %s' %(x.shape, y.shape))\n",
    "\n",
    "alpha = 0.01 # learning rate\n",
    "\n",
    "# call gredient decent, and get intercept(=theta0) and slope(=theta1)\n",
    "theta0, theta1 = gradient_descent(alpha, x, y, ep=0.001, max_iter=1000)\n",
    "print('theta0 = %s theta1 = %s' %(theta0, theta1))\n",
    "\n",
    "# check with scipy linear regression \n",
    "slope, intercept, r_value, p_value, slope_std_error = stats.linregress(x[:,0], y)\n",
    "print('intercept = %s slope = %s' %(intercept, slope))\n",
    "\n",
    "# Hypothesis\n",
    "y_predict = theta0 + theta1*x\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6), dpi=80)\n",
    "plt.title('Linear Regression')\n",
    "plt.scatter(x, y, color='blue', label='training data')\n",
    "plt.plot(x, y_predict, color='red', label='linear regression', linewidth=2)\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a244a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a499291",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x,y)\n",
    "plt.scatter(x,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6103e154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9999999999879006 -0.3333333333058288\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Gradient Descent Algorithm\n",
    "# MSE function is being used as the Cost Function\n",
    "# Goal is to minimize this error value to obtain most accurate values for m and b\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data\n",
    "x = np.array([1,2,3])\n",
    "y = np.array([2,3,6])\n",
    "\n",
    "# Building the Gradient Descent function\n",
    "def gradient_descent(x, y):\n",
    "    m = 0 # Slope\n",
    "    b = 0 # y-intercept\n",
    "    \n",
    "    n = float(len(x)) # Sample size of training set\n",
    "    \n",
    "    alpha = 0.1 # Learning rate\n",
    "    epochs = 1000 # Number of iterations to perform Gradient Descent\n",
    "    \n",
    "    # Performing Gradient Descent\n",
    "    for i in range(epochs):\n",
    "        y_pred = m*x + b # Current predicted value of y\n",
    "        cost = (1/n) * sum([val**2 for val in (y - y_pred)]) # MSE function is being used as the Cost Function\n",
    "        Dm = (-2/n) * sum(x*(y - y_pred)) # Partial Derivative with respect of m\n",
    "        Db = (-2/n) * sum(y - y_pred) # Partial Derivative with respect of b\n",
    "        m = m - alpha*Dm # Updating current value of m\n",
    "        b = b - alpha*Db # Updating current value of b\n",
    "        \n",
    "        # Print results\n",
    "        # print(\"Iteration: {} m = {}, b = {}, error = {}\".format(i, m, b, cost))\n",
    "        \n",
    "    return m, b\n",
    "\n",
    "m, b = gradient_descent(x=x, y=y)\n",
    "print(m, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "57eabd2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x17acb2c47f0>]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = m*x + b\n",
    "\n",
    "plt.scatter(x,y)\n",
    "plt.plot(x,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "df2034bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 6] [1.5 3.  4.5] 0.5 [0.25] 0.08333333333333333\n",
      "[2 3 6] [1.5 3.  4.5] 0.0 [0.25, 0.0] 0.08333333333333333\n",
      "[2 3 6] [1.5 3.  4.5] 1.5 [0.25, 0.0, 2.25] 0.8333333333333333\n"
     ]
    }
   ],
   "source": [
    "m = 1.5 # Slope\n",
    "b = 0 # y-intercept\n",
    "n = float(len(x))\n",
    "\n",
    "y_pred = m*x + b # Current predicted value of y\n",
    "\n",
    "sq = []\n",
    "for val in (y - y_pred):\n",
    "    sq.append(val**2)\n",
    "    J = (1/n) * sum(sq)\n",
    "    print(y, y_pred, val, sq, J)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7c281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(n):\n",
    "    return {\"w\": np.zeros(n), \"b\": 0.0}\n",
    "\n",
    "\n",
    "def predict(x, parameters):\n",
    "    # Prediction initial value\n",
    "    prediction = 0\n",
    "    \n",
    "    # Adding multiplication of each feature with it's weight\n",
    "    for weight, feature in zip(parameters[\"w\"], x):\n",
    "        prediction += weight * feature\n",
    "        \n",
    "    # Adding bias\n",
    "    prediction += parameters[\"b\"]\n",
    "        \n",
    "    return prediction\n",
    "\n",
    "# Used features and target value\n",
    "features = [\"size\"]\n",
    "target = [\"price\"]\n",
    "\n",
    "# Slice Dataframe to separate feature vectors and target value\n",
    "X, y = df_data[features].as_matrix(), df_data[target].as_matrix()\n",
    "\n",
    "# Initialize model parameters\n",
    "n = len(features)\n",
    "model_parameters = init(n)\n",
    "\n",
    "# Make prediction for every data sample\n",
    "predictions = [predict(x, model_parameters) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288751a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Original Dataset\n",
    "# x = np.array([1,2,3,4,5,6,7,8,9,10,11,12])\n",
    "# y = np.array([2,5,4,7,6,9,11,12,15,13,16,18])\n",
    "\n",
    "x = np.array([1,2,3,4,5])\n",
    "y = np.array([2,5,4,7,6])\n",
    "\n",
    "# Theta values\n",
    "t0 = 0 # Intercept\n",
    "t1 = 0 # Slope\n",
    "\n",
    "# Variables\n",
    "max_iter = 5 # Number of iterations\n",
    "m = len(y) # Sample size\n",
    "alpha = 0.01 # Learning rate\n",
    "\n",
    "\n",
    "\n",
    "# Loop through iterations\n",
    "# for i in range(max_iter):\n",
    "#     y_pred = t0 + t1*x\n",
    "#     J = (1/2*m) * sum([val**2 for val in (y_pred-y)])\n",
    "#     print(J)\n",
    "#     J2 = sum([(y_pred - y)**2 for i in range(m)])\n",
    "#     print(J2)\n",
    "    \n",
    "# Hypothesis\n",
    "y_pred = t0 + t1*x\n",
    "print('y_pred:', y_pred)\n",
    "print('y_actual:', y)\n",
    "print('derivative:', y_pred-y)\n",
    "\n",
    "J = 0\n",
    "for i,j in enumerate(y_pred-y):\n",
    "    d = j**2\n",
    "    print('d'+str([i])+':', d)\n",
    "    J += d\n",
    "    print('J'+str([i])+':', J,'\\n')\n",
    "\n",
    "print(J)\n",
    "\n",
    "# J = (1/2*m) * sum([val**2 for val in (y_pred-y)])\n",
    "# print(J)\n",
    "\n",
    "# Plot\n",
    "plt.scatter(x,y);\n",
    "plt.plot(x,y_pred);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81af988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18576d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def gradient_descent(x,y):\n",
    "    t0, t1 = 0, 0 # initial theta\n",
    "    max_iter = 10000 # number of iterations\n",
    "    m = len(y) # sample size\n",
    "    alpha = 0.01 # learning rate\n",
    "    \n",
    "    # loop through iterations\n",
    "    for i in range(max_iter):\n",
    "        y_pred = t0 + t1*x\n",
    "        cost = (1/m) * sum([val**2 for val in (y_pred-y)])\n",
    "        grad0 = -(2/m) * sum(y-y_pred)\n",
    "        grad1 = -(2/m) * sum((y-y_pred)*x)\n",
    "        \n",
    "        t0 = t0 - alpha * grad0\n",
    "        t1 = t1 - alpha * grad1\n",
    "        print(\"t0 {}, t1 {}, cost {}, iteration {}\".format(t0, t1, cost, i))\n",
    "        \n",
    "    return t0, t1\n",
    "\n",
    "# Original Dataset\n",
    "x = np.array([1,2,3,4,5,6,7,8,9,10,11,12])\n",
    "y = np.array([2,5,4,7,6,9,11,12,15,13,16,18])\n",
    "\n",
    "gradient_descent(x,y)\n",
    "# (0.7878787878787747, 1.3916083916083932)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4323349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning values to theta0, theta1\n",
    "theta0, theta1 = gradient_descent(x,y)\n",
    "\n",
    "# Hypothesis Function\n",
    "y_predict = theta0 + theta1*x\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6), dpi=80)\n",
    "plt.title('Linear Regression')\n",
    "plt.scatter(x, y,  color='blue', label='training data')\n",
    "plt.plot(x, y_predict, color='red', label='linear regression', linewidth=2)\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c12a835",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
